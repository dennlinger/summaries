From running the Cleaner utility with 20 (summ) / 50 (reference) char length minima.

11 samples were removed from the dataset.
Breakdown by filter category:
Reason 'too_short': 4 samples removed.
Reason 'identity_sample': 0 samples removed.
Reason 'longer_summary': 0 samples removed.
Reason 'ngram_range': 0 samples removed.
Reason 'duplicate': 7 samples removed.
len(clean_klexikon[0])
Out[3]: 2341
len(klexikon["train"])
Out[4]: 2350
len(clean_klexikon[1])
Out[5]: 273
len(klexikon["validation"])
Out[6]: 274
len(klexikon["test"])
Out[7]: 274
len(clean_klexikon[2])
Out[8]: 273

With 1.25 min CR:
14 samples were removed from the dataset.
Breakdown by filter category:
'reference_too_short': {'train': 0, 'validation': 0, 'test': 0} samples removed across splits.
'summary_too_short': {'train': 0, 'validation': 0, 'test': 0} samples removed across splits.
'identity_sample': {'train': 0, 'validation': 0, 'test': 0} samples removed across splits.
'compression_ratio': {'train': 10, 'validation': 1, 'test': 1} samples removed across splits.
'extractiveness': {'train': 0, 'validation': 0, 'test': 0} samples removed across splits.
'exact_duplicate': {'train': 0, 'validation': 0, 'test': 0} samples removed across splits.
'both_duplicate': {'train': 0, 'validation': 0, 'test': 0} samples removed across splits.
'summary_duplicate': {'train': 0, 'validation': 0, 'test': 0} samples removed across splits.
'reference_duplicate': {'train': 2, 'validation': 0, 'test': 0} samples removed across splits.
& train & $2346$ & $0$ & $0$ & $0$ & $10$ & $0$ & $0$ & $2$ & $0$ & $2334$ & $(99.49\%)$ \\
& validation & $273$ & $0$ & $0$ & $0$ & $1$ & $0$ & $0$ & $0$ & $0$ & $272$ & $(99.63\%)$ \\
& test & $274$ & $0$ & $0$ & $0$ & $1$ & $0$ & $0$ & $0$ & $0$ & $273$ & $(99.64\%)$ \\

